@store.search(#ness.IsThing("x","y"))
tokenizes to:
			@store			-store token
								-has name "store"
								-is operand (meaning operations can take place on it)
			.search			-operation token
								-has operation name "search"
								-has operation ??
								-is operand (meaning operations can take place on it)
			(				-params token
			#ness			-ness token
								-has ness "ness"
								-is operand (meaning operations can take place on it)
			.IsThing		-operation token
								-has operationname "IsThing"
			(				-params token
			"x"				-literal token
								-is operand (meaning operations can take place on it)
			,				-item token
			"y"				-literal token
								-is operand (meaning operations can take place on it)
			)				-endparams token
			)				-endparams token

in a prefix tokenizer scheme, the token(izer) type is determined by the prefix
"@" - parses store name to "."
"." - parses operation name to "(" or "."
"(" - parses nothing (eg. from "(" to itself) , just a placeholder indicating params
"," - parses nothing, just a placeholder for params separator
")" - parses nothing, just a placeholder indicating params have closed
"#" - parses ness name to "."
everything else parses to literal

how does the lexical parsing work?
using the forward moving tokenizer sig:
	bool Parse(string text, int currentPosition, object state, IToken currentToken,
            out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser);

in forward moving tokenizing, each tokenizer says what the next tokenizer is.  since we're using prefixing
we can provide a default tokenizing behaviour to the tokenizer such that the next tokenizer to use 
is determined by the first character. 

perhaps a set of tokenizer decorations will do this.  maybe we can use decorators to extend the behaviour 
of the token(izer)s and define the parsing rules this way.  YES! YES! WE CAN

	we can actually do a lot of things from providing exact parsing, to loose/fuzzy parsing even and this may be 
	sufficient enough to do the job.  layer things in via decorations. 

example decorations:

	//this decoration knows how to determine what tokenizer to use for a given prefix
	//it uses this knowledge to decide what the next parser is.  This overrides default behaviour.
	IPrefixRoutingTokenizer : ITokenizer
	{
		ITokenizer GetTokenizerFromPrefix(string prefix);
	
		bool Parse(string text, int currentPosition, object state, IToken currentToken,
            out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser)
			{
				bool rv = base.Parse(text, currentPosition, state, currentToken, newPosition, newToken, newParser);
				var newParser = GetTokenizerFromPrefix(text[newPosition]);
				return rv;
			}

		//fluent usage: tokenizer.HasPrefixRouting(Func<string,IForwardMovingTokenizer> routingFunction);
	}

	//will only tokenize if the prior token is cool
	IPriorTokenValidating : ITokenizer
	{
		bool IsPriorTokenValid(IToken priorToken);
		bool Parse(string text, int currentPosition, object state, IToken currentToken,
            out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser)
			{
				if(!IsPriorTokenValid(currentToken))
					throw new InvalidLexicon("invalid prior token");

				bool rv = base.Parse(text, currentPosition, state, currentToken, newPosition, newToken, newParser);
				return rv;
			}
	
		//fluent usage: tokenizer.Follows(IForwardMovingTokenizer tokenizer);
	}

	//knows whether it can handle the current job
	ISelfDirectedTokenizer : ITokenizer
	{
		bool CanHandle(string text, int currentPosition);
		bool Parse(string text, int currentPosition, object state, IToken currentToken,
            out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser)
			{
				if(!CanHandle(text, currentPosition))
					throw new InvalidLexicon("cannot handle phrase");

				bool rv = base.Parse(text, currentPosition, state, currentToken, newPosition, newToken, newParser);
				return rv;
			}

		//fluent usage: tokenizer.Validates(Func<string, int> canHandleStrategy);
	}

	//requires a prefix to tokenize
	IPrefixedTokenizer : ITokenizer, ISelfDirectedTokenizer
	{
		string[] Prefixes {get;}
		bool canHandle(string text, int currentPosition)
		{
			foreach(string each in Prefixes)
				if(text.substring(currentPosition).startswith(each))
					return true;

			return false;
		}
				
		bool Parse(string text, int currentPosition, object state, IToken currentToken,
            out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser)
			{
				if(!CanHandle(text, currentPosition))
					throw new InvalidLexicon("cannot handle phrase");

				bool rv = base.Parse(text, currentPosition, state, currentToken, newPosition, newToken, newParser);
				return rv;
			}

		//fluent usage: tokenizer.HasPrefix(params string[] prefixes);
	}

	//parses from current spot to any of the suffixes
	ISuffixedTokenizer : ITokenizer
	{
		string[] Suffixes{get;}
		bool Parse(string text, int currentPosition, object state, IToken currentToken,
            out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser)
			{
				//for all the suffixes this parses to, finds the nearest one (nongreedy)
				int idx = -1;
				foreach(var each in Suffixes)
				{
					var tempIdx = text.substring(currentPosition).indexOf(each);
					if(tempIdx == -1)
						continue;
					if(idx == -1)
						idx = tempIdx; continue;

					if(tempIdx < idx)
						idx = tempIdx; continue;
				}

				newPosition  = idx;
				newToken =  Token.New(...  );//ur std simple token
				newParser = null;
			}

			//fluent usage: tokenizer.HasSuffix(params string[] suffices);
	}


	INamedTokenizer : ITokenizer <IHasId<string>>
	{
		bool Parse(string text, int currentPosition, object state, IToken currentToken,
            out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser)
			{
				//parse as usual
				bool rv = base.Parse(text, currentPosition, state, currentToken, newPosition, newToken, newParser);
				newToken.HasTokenizer(this.Id);
			}

			//fluent usage: tokenizer.HasName(string name);
	}

	//this decoration uses a router to get next tokenizer
	IGetsNextWithRoutingTokenizer: ITokenizer
	{
		IRoutingTokenizer Router {get;}
		//if the default behaviour returns the next tokenizer, override it
		bool OverrideIfNonNull {get;}

		bool Parse(string text, int currentPosition, object state, IToken currentToken,
        out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser)
		{
			//parse as usual
			bool rv = base.Parse(text, currentPosition, state, currentToken, newPosition, newToken, newParser);
			
			if(newParser == null)
			{
				newParser = Router.GetTokenizer(text, newPosition);
				return rv;
			}
			else
			{		
				//override the new parser to use the router
				if(OverrideIfNonNull)
				{
					newParser = Router.GetTokenizer(text, newPosition);
					return rv;
				}
			}
		}
	
		//fluent usage: tokenizer.GetsNextWith(IRoutingTokenizer router, bool overrideIfNonNull = false);
	}

	//this decoration delegates the actual tokenizing process to the appropriate tokenizer
	IRoutingTokenizer : ITokenizer
	{
		IStoreOf<ITokenizer> Rules {get;}
		ITokenizer AddTokenizer(ITokenizer t)
		{
			//tell each tokenizer to use the router as the backup router
			var newT =	t.GetsNextWith(this, false);
		
			Rules.Save(newT)
			return newT;
		}

		ITokenizer GetTokenizer(string text, int currentPosition)
		{
			List<ITokenizer> tokenizers = Rules.GetAll();

			//iterate thru all the tokenizers and find ones that know if they can handle stuff
			foreach(var each in tokenizers)
			{
				if(each is ISelfDirectedTokenizer)
				{
					if((each as ISelfDirectedTokenizer).CanHandle(text, currentPosition))
						return each;
				}	
			}
			return null;
		}

		bool Parse(string text, int currentPosition, object state, IToken currentToken,
        out int newPosition, out IToken newToken, out IForwardMovingTokenizer newParser)
		{
			//get the new tokenizer
			var tokenizer = GetTokenizer(text, currentPosition);
			Condition.Requires(tokenizer).IsNotNull();

			//delegate to it
			bool rv = tokenizer.Parse(text, currentPosition, state, currentToken, newPosition, newToken, newParser);
			return rv;
		}
		
		//fluent usage: tokenizer.HasPrefixRouting(Func<string,IForwardMovingTokenizer> routingFunction);
	}
	//have NaturallyNotImplemented hollow core

	//so lets use these decorations to define the parsing rules of a function call: @thing.function(arg1,arg2)
	IRoutingTokenizer router = new();
	
	var thingTokenizer = NaturallyNotImplementedTokenizer.New().HasPrefix("@").HasSuffix(".").HasName("thing");
	var functionTokenizer = NaturallyNotImplementedTokenizer.New().HasPrefix(".").HasSuffix("(").HasName("function");
	var argTokenizer = NaturallyNotImplementedTokenizer.New().HasPrefix("(", ",").HasSuffix(",",")").HasName("arg");
	router.AddTokenizer(thingTokenizer).AddTokenizer(functionTokenizer).AddTokenizer(argTokenizer);

	//and we're done.  at this point it should be sufficient to tokenize a simple function call

	//and if we wanted to add some goofy extra rules
	functionTokenizer.Follows(thingTokenizer);
	argTokenizer.Follows(functionTokenizer);

	/*
	to implement these rules:
	
	for:  @store.search(#ness.IsThing("x","y"))
	
	"@" - parses store name to "."
	"." - parses operation name to "(" or "."
	"(" - parses nothing (eg. from "(" to itself) , just a placeholder indicating params
	"," - parses nothing, just a placeholder for params separator
	")" - parses nothing, just a placeholder indicating params have closed
	"#" - parses ness name to "."
	everything else parses to literal
	
		this example covers nested tokens as well
	*/

	IRoutingTokenizer router = new();
	var storeTokenizer = BlankTokenizer.New().HasPrefix("@").HasSuffix(".").HasName("store");
	var opTokenizer = BlankTokenizer.New().HasPrefix(".").HasSuffix(".","(").HasName("op");
	var openParenTokenizer = BlankTokenizer.New().HasPrefix("(").HasSuffix("(").HasName("openParen");
	var itemTokenizer = BlankTokenizer.New().HasPrefix(",").HasSuffix(",").HasName("item");
	var closeParenTokenizer = BlankTokenizer.New().HasPrefix(")").HasSuffix(")").HasName("closeParen");
	var nessTokenizer = BlankTokenizer.New().HasPrefix("#").HasSuffix(".").HasName("ness");
	var functionTokenizer = BlankTokenizer.New().HasPrefix(".").HasSuffix("(").HasName("function");
	var argTokenizer = BlankTokenizer.New().HasPrefix("(", ",").HasSuffix(",",")").HasName("arg");
	router.AddTokenizer(all the above)


	STEP 2 - COMPILATION!!

	@store.search(#ness.IsThing("x","y"))
	tokenizes to:
			@store			store
			.search			op
			(				openParen
			#ness			ness
			.IsThing		op
			(				openParen
			"x"				literal
			,				item
			"y"				literal
			)				closeParen
			)				closeParen

	compilation will perform a forward only move of this tokenset and convert it into a series of UnitsOfWork to execute in order.
	
		UnitOfWork has:
			-Operand (can be: store, op, ness, literal)
			-Operation (can be op).  This op will correspond to some function to perform on the operand with the args, returning 
			-Args (list of literal or UnitOfWork)
			-Eval
			-ChainToSibling()

		compilation process goes like this:
		step:						current UoW:							UoW stack:
		@store						UoW1.operand=store							1
		.search						UoW1.operation=search
		(							UoW1.args.add(UoW2)							2
		#ness						UoW2.operand=#ness
		.IsThing					UoW2.operation=IsThing
		(							UoW2.args.add(UoW3)							3
		"x"							UoW3.eval = "x", it's a literal (dequeue)	2	
		,							UoW2.args.add(UoW4)							3
		"y"							UoW4.eval = "y" (dequeue)					2
		)							UoW2.eval()		(dequeue)					1
		)							UoW1.eval()		(dequeue)					0								
									
		ok, so every time an eval() call happens we have to replace the UoW that generated the value,
		with the actual value, on the parent's arg list.  by the time we're back at the last UoW,
		we'll all have actual values (or, deferred getters which means we can avoid evaluation until we need to,
		and in this way we can probably queue up the sequence of operations on the fly)

